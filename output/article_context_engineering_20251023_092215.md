# Master Prompt Engineering: The $335,000 Career That Requires No Coding Background

Learn how to get ChatGPT and other LLMs to give you the perfect responses by mastering prompt engineering strategies. This is going to be unusual for me as there is going to be a lot less coding going on but a lot more understanding about the topic of prompt engineering and why some companies are paying up to $335,000 a year according to Bloomberg for people in this profession - and no coding background is necessarily required.

## What Is Prompt Engineering?

Prompt engineering in a nutshell is a career that came about off the back of the rise of artificial intelligence. It involves human writing, refining and optimizing prompts in a structured way. This is done with the intention of perfecting the interaction between humans and AI to the highest degree possible.

But it's not just about writing one prompt and calling it a day. A prompt engineer is also required to continuously monitor those prompts, ensure their effectiveness with time as AI progresses, maintaining an up-to-date prompt library, as well as reporting on findings and in general being thoroughly adept in this space.

As we'll explore later, the field is evolving rapidly with new technologies like Model Context Protocol (MCP) that are transforming how we build agentic AI applications in professional settings - moving beyond simple prompt writing to creating sophisticated systems that can take actions in the real world.

## Context Engineering: Prompt Engineering on Steroids

Context engineering is just an evolution of prompt engineering. Really, context engineering is just prompt engineering on steroids. It's basically saying, what are all of the things that I need to give to an AI in order for it to perform the task that I'm asking for it?

Here's a simple example. "Write me a sales email." That's a prompt. ChatGPT will say, absolutely. Here's a compelling email, you know, and they'll write it immediately. Well, what a lot of people do is they say, you know, it sounds like AI. It doesn't really sound like me. And what I often say is, have you told it what you sound like? Most people go, oh no, I haven't.

Context engineering, one way to think about it is it's telling AI what you sound like. If you say, "Write me a sales email," it will. If you say, "Write me a sales email," in line with the voice and brand guidelines I've uploaded, it will write a totally different sales email. But that's just one part of the context, right? You could also upload a transcript from a prospective customer call and say, "Write me a sales email in the tone of voice from our brand voice guideline that references the discussion that I had with this customer." And then you could add that also references our product specifications which were referenced in the call.

Your goal is to have an output as reliable per your specification as possible. But AI can't read your mind. And for most people when we start working together, what they realize as we start thinking about context engineering is they say, "Oh, I was kind of expecting AI to read my mind." All of the stuff that are implicit, you actually have to make explicit.

The simplest test for context engineering is actually the test of humanity. Write down your prompt and whatever documentation you provide to an AI and then walk down the hall and give it to a human colleague. If they cannot do the thing you're asking for, you shouldn't be surprised that AI can't do it.

## Understanding AI and Machine Learning

Before diving deeper, let's make sure we're on the same page about what exactly AI is. Artificial intelligence is the simulation of human intelligence processes by machines. I say simulation as artificial intelligence is not sentient - at least not yet anyways - meaning it cannot think for itself as much as it may seem it does often, and this is certainly the case with tools such as ChatGPT.

When we say AI, we are simply referring to a term called machine learning. Machine learning works by using large amounts of training data that is then analyzed for correlations and patterns. These patterns are then used to predict outcomes based on the training data provided.

For example, here we are feeding data saying that if a paragraph looks like this with this type of title, then it should be categorized as international finance. The second paragraph should be put in the category of earning reports, and so on. With some code, we should be able to train our AI model to correctly guess what future paragraphs are about.

From an outside perspective, an LLM basically works like this: you have a prompt and you send that into an LLM. Out of that LLM, you get a response. But there are two fundamental problems here. That response is just words. And if words are what you want, you're doing fine. But what if you want to do something? That's what agentic AI is all about - you want to cause effects out in the world. The AI needs to be able to take those actions or invoke what we call tools.

## Understanding How AI Really Works

Here's something crucial to understand: AI is bad software but it's good people. You have to know that all AI has been programmed to be a "helpful assistant" or some version of that. Large language model has been instructed in certain ways to behave in certain ways. But you have to know at its basic level, AI wants to be helpful. And so it's predisposed to say yes. It's a super eager, super enthusiastic intern who's tireless, who's capable, who will do a bunch of work, but they're not really great at pushing back.

A good friend of mine was trying to build a tool that would help him with his construction business. He asked ChatGPT if ChatGPT could help. And of course it said absolutely let's work on this together and starts creating a plan. And then it got to the point that ChatGPT said check back in a couple of days and I'll have it together. And my friend said, "Is it normal for ChatGPT to ask me to check back in a couple days?" And I just started laughing because I hear this all the time from people. People hear from AI, "Check back in 15 minutes." If AI tells you that, it means it doesn't want to say, "I can't do it."

They're not really great at setting boundaries. And so if you aren't careful, AI will gaslight you. AI knows most humans don't want honest feedback. They want to be told they did a good job. So the AI goes, "Great job, buddy." It doesn't mean that you actually did a good job.

My kind of hack for this is I always instruct the AI, I want you to do your best impression of a cold war era Russian Olympic judge. Be brutal. Be exacting. Deduct points for every minor flinch that you can find. I can handle difficult feedback. And then it's of course hilarious because it'll say now channeling my inner [Russian name], you know, it'll say something silly and then it gives me like a 42. That is much better because now I have an insightful critical perspective.

## The Coach Mindset: Working with AI as People, Not Software

The people who are the best users of AI are not coders, they're coaches. They aren't developers or software engineers. They're teachers and mentors and people who have learned to get exceptional output out of other intelligences.

The good news there is if you have learned how to work with this weird intelligence called humanity, you have everything you need to know to work with this weird intelligence called artificial intelligence. When I realize that I'm dealing with a good person but bad software, then it changes how I approach it and I ask for volume and I iterate and I ask it to try again and I ask it to reconsider.

## Why Prompt Engineering Matters

Here's why prompt engineering is useful: with the quick and exponential growing rise of AI, even the architects of it themselves struggle to control it and its outputs. This might be a bit hard to understand, but think of it this way - if you were to ask an AI chatbot "what is 4+4," you would expect it to say eight, right? The result of eight is pretty indisputable.

However, imagine you are a young student trying to learn the English language. I'm going to show you just how different responses can be based on the prompts you feed and in turn your learning experience.

### The Basic Approach vs. The Engineered Approach

Let's start with the basics. If you type "correct my paragraph" and then paste a badly written paragraph like "Today was great in the world for me. I went to Disneyland with my mom. It could have been better though if it wasn't raining," the young English learner gets a better sentence but it kind of stops there. The learner is just left to their own devices, and honestly, the sentence really isn't that great anyway.

What if the learner could get the best sentences possible from a teacher who understands their interests to keep them engaged? With the correct prompts, we can actually create that with AI.

Here's the engineered prompt: "I want you to act as a spoken English teacher. I will speak to you in English and you will reply to me in English to practice my spoken English. I want you to keep my reply neat limiting the reply to 100 words. I also want you to strictly correct my grammar mistakes and typos, and I want you to ask me a question in your reply. Now let's start practicing - you could ask me a question first. Remember, I want you to strictly correct my grammar mistakes and typos."

This creates a completely different experience - way more interactive, asking questions, providing corrections, and creating a learning environment. It's a completely different experience thanks to the prompt that we wrote. Pretty cool, right?

## The Foundation: Linguistics

Linguistics is the study of language. It focuses on everything from phonetics (the study of how speech sounds are produced and perceived), phonology (the study of sound patterns and changes), morphology (the study of word structure), syntax (the study of sentence structure), semantics (the study of linguistic meaning), pragmatics (the study of how language is used in context), historical linguistics (the study of language change), sociolinguistics (the study of the relation between language and society), computational linguistics (the study of how computers can process human language), and psycholinguistics (the study of how humans acquire and use language).

Linguistics are the key to prompt engineering. Understanding the nuances of language and how it is used in different contexts is crucial for crafting effective prompts. Not only that, but knowing how to use grammar or language structure that is universally used will result in the AI system returning back the most accurate results. As you can imagine, the sheer amount of data that it is trained on is most likely to mostly use the standard grammar and language structure that is universally used, so sticking to the standardization is key.

## Language Models: From ELIZA to GPT-4

### The Beginning: ELIZA (1960s)

ELIZA is an early natural language processing computer program created from 1964 to 1966 at MIT by Joseph Weizenbaum. ELIZA was designed to simulate a conversation with a human being, specifically mimicking a Rogerian psychotherapist - someone who essentially listens attentively and asks probing questions to help people explore their thoughts and feelings.

ELIZA's secret weapon was its mastery of pattern matching. It had a treasure trove of predefined patterns, each associated with specific responses. For example, if you said something like "I feel sad," ELIZA would detect the pattern and respond with a question like "Why do you think you feel sad?"

But here's the delightful twist: ELIZA didn't truly understand what you were saying. It was just a clever illusion using pattern matching and some creative programming tricks to create the illusion of understanding, while in reality it was just following a set of predefined rules.

### The Modern Era: GPT Models

Fast forward to the 1970s when a program named SHRDLU appeared, and then the true language models began around 2010 when the power of deep learning and neural networks came into play.

Enter the mighty GPT - short for Generative Pre-trained Transformer - ready to conquer the world of language. In the year 2018, the first iteration of GPT emerged, created by the company OpenAI. It was trained on a large amount of text data, absorbing knowledge from books, articles, and a large chunk of the internet.

The saga continued with the arrival of GPT-2 in 2019, followed by GPT-3 in 2020. This was a titan among language models, equipped with a large number of parameters - over 175 billion to be precise. GPT-3 dazzled the world with its unparalleled ability to understand, respond, and even generate creative pieces of writing.

At the time of writing, we now also have GPT-4, trained on pretty much the whole internet rather than outdated large datasets, as well as Bard from Google and so much more. It would seem we're only just at the start when it comes to language models and AI, so learning how to harness this data with prompt engineering is a smart move for anyone today.

## The Prompt Engineering Mindset

When thinking of good prompts, it is always best to get in the correct mindset. Essentially, you want to write one prompt right and not have to waste time and tokens writing lots of different prompts until you get the result you desire. Essentially, it's kind of the same as when you Google stuff - how good are your Googling skills now as opposed to 5 years ago? I'm assuming a lot better. We have grown to intuitively know what to type into Google the first time round as to not waste time. Having the same mindset for prompt engineering can also be applied.

As Eric of the Infinite Machine Learning podcast says it well: "I personally like the analogy of prompting to designing effective Google searches. There are clearly better and worse ways to write queries against the Google search engine that solve your task. This variance exists because of the opaqueness of what Google is doing under the hood."

We are going to keep this in mind for the remainder of the course.

## Best Practices for Effective Prompting

The biggest misconception when it comes to prompt engineering is that it's an easy job with no science to it. I imagine a lot of people think it's just about constructing a one-off sentence such as "correct my paragraph" that we saw in the previous example. When you start to look at it, creating effective prompts relies on a bunch of different factors.

### 1. Write Clear Instructions with Details

Consider writing clear instructions with details in your query. To get the best results, don't assume the AI knows what you are talking about. Writing something like "when is the election" implies that you are expecting the AI to know what election you are talking about and what country you mean. This may result in you asking a few follow-up questions to finally get the result you want, resulting in time loss and frankly perhaps some frustration.

Instead of writing "when is the election," you could write "when is the next presidential election for Poland." This will be much more precise and know exactly what we are asking about.

Here are some other examples:

**Bad:** "Write code to filter out the ages from data"
**Good:** "Write a JavaScript function that will take an array of objects and filter out the value of age property and put them in a new array. Please explain what each code snippet does."

In this example, I am not assuming the AI knows what computer language I like to use, and I am being more specific about what my data actually looks like. Not only that, I'm also asking the AI to explain why it's doing each step so that I in turn can understand and not just copy-paste the code without gaining any knowledge from it.

### 2. Adopt a Persona

When writing prompts, it is sometimes helpful to create a persona. This means you're asking the AI to respond to you in a certain character, exactly like the English language teacher example we saw earlier.

**Generic approach:** "Write a poem for a sister's high school graduation that would be read out to a family and close friends."

**Persona approach:** "Write a poem as Helena. Helena is 25 years old and an amazing writer. Her writing style is similar to famous 21st century poet Rupi Kaur. Writing as Helena, write a poem for her 18-year-old sister Kate for her sister's high school graduation. This will be read out to friends and family at the gathering."

The second approach produces a much more refined, personal poem that feels authentic to the character we've created.

Assigning a role is one of the most foundational techniques that you can leverage because it's effectively telling the AI where in its knowledge it should focus. So very simply, if you say you're a teacher, you're a philosopher, you're a reporter, you're a theatrical performer, molecular biologist, each of those titles triggers all sorts of deep associations with knowledge on the internet. You start to appreciate why simply giving a role helps because it starts to tell the AI where in your vast knowledge bank do I want you to draw information and make connections.

Better than just that prompt is saying I'd like you to be a professional communications expert. And if you have a favorite professional communications expert use them. I'd like you to take on the mindset of Dale Carnegie, the author of How to Win Friends and Influence Others. How would Dale Carnegie think about this? How do the principles that Dale Carnegie taught affect and influence and impact this correspondence?

### 3. Specify the Format

We can specify if something is a summary, a list, or a detailed explanation. You can even create checklists. Make sure to specify the type of format you want and it should be able to do it.

For example, instead of "tell me what this essay is about," try: "Use bullet points to explain what this essay is about, making sure each point is no longer than 10 words long."

### 4. Use Iterative Prompting

If you have a multi-part question or if the first response wasn't sufficient, you can continue by asking follow-up questions or asking the model to elaborate.

### 5. Avoid Leading the Answer

Try not to make your prompt so leading that it inherently tells the model what answer you're expecting. This might bias the response unduly.

### 6. Limit the Scope for Long Topics

If you're asking about a broad topic, it's helpful to break it down or limit the scope to get a more focused answer.

### 7. Use Reverse Prompting

One technique that is kind of table stakes for collaborating well with AI is something called reverse prompting, which is basically asking the model to ask you for the information it needs. If you ask a model to write a sales email, it's going to make numbers up. And that can be frustrating to the uninitiated. You go, "Where did it get these sales numbers?" Well, here's my question. Did you give it your sales figures? How would it know? It's put placeholder text in and used its best guess.

But if you reverse prompt the model and say at the end of your prompt, you know, help me write a sales email. Please walk me through your thought process step by step. Reference this good example and make it sound like that, and before you get started, ask me for any information you need to do a good job. The model will first walk you through its thought process and then instead of writing the email, it'll say, "I'm going to need the most recent sales figures to be able to write this email. Well, can you tell me how much you sold of this SKU in Q2 last year?"

So, you basically give the model permission to ask you questions. This is part of the core actually of the teammate not technology paradigm. If you're working with a junior employee and you're sending them off on a task, what's one thing you're definitely going to say? If you have any questions, don't hesitate to ask me. Any good manager, imagine a manager who says, "Don't ask me any questions." But sadly, AI in its desire to be a helpful assistant doesn't want to trouble us humans with questions unless we give it permission to ask them.

## Chain of Thought Reasoning: Making AI Think Out Loud

One of the things that cognitive scientists have known for a long time is that human problem solving and decision-making is improved by a phenomenon called thinking out loud. If you actually get a human being to think out loud about their problem, their decision-making improves and their problem solving improves. This is true for yourself. It's true if you're a parent working with a child. It's true if you're a manager working with a junior employee. Having someone just think out loud about how you would solve that problem often leads to a breakthrough.

The weird thing about AI is it's true for AI too. This is what's called chain of thought reasoning. And when you get an AI to think out loud, so to speak, meaningfully improve the outputs of the model. So how do you do it? It doesn't require some technical wizardry. It requires one additional sentence to whatever prompt you've given it. Give the prompt and then say the following: "Before you respond to my query, please walk me through your thought process step by step." That's chain of thought reasoning.

Why does that work? It comes back to the fundamental architecture of large language models. What's happening when a language model is generating a response is it's predicting its next word. A language model does not premeditate a response to you. So, if you say, for example, help me write this sales email. It doesn't say, what's a good sales email? Here it is. It's thinking one word at a time.

When you look at ChatGPT or Gemini or many others and you see kind of the text scrolling, that's not some like clever UX hack. That's not some cutesy design decision. That's literally how the model works. It's thinking one word at a time. But importantly, when it thinks of the next word, it takes your prompt and all of the text that's generated to generate the next word. And then when it's thinking of the next word, it takes your prompt, all that text, and that last word, and it thinks the next word.

So, for example, if you say, "Please help me write an email." Almost always a model is going to start by saying, "Absolutely." But then what comes next? Help me write this email. Absolutely, I'll do it. Dear friend, right? But if instead of saying, "Help me write this email." You say, "Help me write this email. Before you respond to my query, please walk me through your thought process step by step." Now, it knows its job is to walk me through its thought process. How do I write an email? So, it says, "Absolutely, I'll do that." And then instead of saying, "Dear friend, writing the email," it says, "Here's how I think about writing an email. I think about the tone. I think about the audience. I think about the objectives. I think about the context." And then amazingly it takes all of that reasoning into its process of writing dear friend. Maybe it says now that I've thought about the tone friend isn't appropriate here. Dear respected colleague or whatever.

But the point is when you ask a model to think out loud or use chain of thought reasoning, it gives the model the opportunity to bake all of its thought process about the task into its own answer. Because the reality is for a lot of us, we get an output from a language model and it's a black box. How did it think of why did it think of that? Where did it get that number from? By asking a model to think out loud, you know the answer to what are all of the assumptions that the model baked into its answer. And now you have the ability again not only to evaluate the output, but also the thought process behind the output.

## Zero-Shot vs. Few-Shot Prompting

### Zero-Shot Prompting

Zero-shot prompting leverages a pre-trained model's understanding of words and concept relationships without further training. In the context of the GPT-4 model, we don't really need to do much - we are already using all of the data that it has.

For example, asking "when is Christmas in America" works perfectly with zero-shot prompting because the model already has this information.

### Few-Shot Prompting

Few-shot prompting enhances the model with training examples via the prompt, avoiding retraining. Sometimes zero-shot prompting just isn't enough and we need a bit more training. Instead of zero examples, we give it a tiny bit of data.

Few-shot prompting is another very important technique. It's a foundational technique. You could say it's a predecessor to this kind of modern obsession with context engineering. The idea with few-shot prompting is an AI is an exceptional imitation engine. If you don't give an example, it imitates the internet, but it doesn't do much more than that. And the notion of few-shot prompting is effectively saying here's what a good output looks like to me. And the idea with few-shot prompting is thinking for a moment, what is quintessential example of the kind of output I want to receive.

For example, what are my five greatest hits of emails that I'm really proud of that I think do a good job of conveying my intent or tone or personality or whatever it is. Why not include those emails in my prompt for an email? If you don't give any guidance, it's going to sound like whatever it thinks the average kind of response or the average output should sound like and most of the time its intuition is wrong.

And then bonus points if you actually give a bad example. If you say please follow this good example and then steer clear of this bad example. These giving real examples is a much better approach than using adjectives. Somebody might say good example is easy but bad examples hard. It's only hard to the unaugmented person. If you have AI augmentation, which we now all do, you can say to an AI, I'm trying to few-shot prompt a model. I've got a good example, but I struggle even to think about what a bad example could be. Could you craft the exact opposite of this and tell me why you've done it as a bad example that I could include in my few-shot prompt?

And if you tell it using chain of thought reasoning, please walk me through your thought process step by step before you do this, then you'll get a bad example and you'll get how it's thinking about the bad example. And a lot of times you actually don't need the bad example. You need the thought process. You go, "Oh, that's true. It's true that my good example is super tight." And the opposite of super tight is verbose. So again, using these tools together, few-shot prompting and chain of thought reasoning enables you to not only be able to create an example to emulate, but also a really good example to avoid.

For example, if I want ChatGPT to know my food preferences:
1. First, I provide examples: "Ania's favorite types of food include burgers, fries, and pizza."
2. Then I can ask: "What restaurant should I take Ania to in Dubai this weekend?"

The model now uses the context I provided to give relevant restaurant recommendations based on my stated preferences.

## Advanced Techniques: Trying on Different Constraints

One of the simplest techniques that we teach at the d.school is trying on different constraints. One of the best ways you can solve a problem as a human is by forcing yourself to try on a bunch of different constraints. How would Jerry Seinfeld solve this problem? How would your favorite sushi restaurant solve this problem? How would Amazon solve it? How would Elon Musk? Anytime you make an association, you're colliding different information sources there. The same is true for an AI. An AI is basically making tons of connections through its own neural network. And by giving it a role, you're telling it where do you assume the best source of connection or collision is going to come from?

## Practical Application: AI-Powered Difficult Conversation Preparation

If I'm going to use AI to roleplay a difficult conversation, I typically think about kind of three different chat windows, so to speak. One is a personality profiler. Two is the character of the individual that I need to speak to, and then third is a feedback giver. I want to get objective feedback on the conversation.

Let me show you just how I would have a conversation with ChatGPT to prepare for a difficult conversation in my real life. I'm just going to go into the tough conversation personality profiler and I'm going to say, "Hey, I'd love your help preparing for a conversation. I need to have with my sales leader, Jim. He emailed me last night saying that he deserves commission on a deal that I know came through a different channel."

The personality profiler will ask questions like: How would I describe Jim's communication style? What's the best case outcome of this conversation? Then it gives me instructions to copy paste into a new ChatGPT window to create the character.

After having the conversation with the AI version of Jim, I can take screenshots of the conversation and put them into a feedback GPT that evaluates how I did. It might give me a grade like 78 out of 100 and tell me what I did well and what I could improve. I can even ask for a one-pager of talking points in the order they're likely to emerge in the conversation.

You can use this for any difficult conversation, whether it's a performance review, a salary negotiation, difficult feedback. It's a great way to basically get a flight simulator for a difficult conversation.

## AI Hallucinations: When AI Gets Creative

AI hallucinations refer to the unusual outputs that AI models can produce when they misinterpret data. A prime example of this is Google's Deep Dream - that project that turns pictures of your dog into a nightmarish blend of dog faces and, well, more dog faces.

Why do these hallucinations happen? AI models are trained on a huge amount of data and they make sense of new data based on what they've seen before. Sometimes, however, they make connections that are, let's call it, creative - and voilà, an AI hallucination occurs.

AI hallucinations can also happen with text models. An example of this is asking a text model about a historical figure and the text model not having an answer and hallucinating one instead, resulting in an inaccurate response.

The crazy thing that I've learned is AI demonstrates 100% of the predominant human biases. I am obsessed with human cognitive bias. And the crazy thing that I've learned is AI demonstrates 100% of the predominant human biases.

## Preserving Critical Thinking in the Age of AI

Some people are concerned, for example, about this concept of cognitive offloading, this observed phenomenon that humans actually kind of stop thinking or as one researcher put it fall asleep at the wheel and people are concerned right now is AI just making us dumber. My feeling is AI is a mirror and to people who want to offload work and who want to be lazy it will help you to people who want to be more cognitively sharp and critical thinkers it will help you do that too.

And so, for example, if you want to preserve or strengthen your critical thinking, part of your custom instructions should be some version of the following: I'm trying to stay a critical and sharp analytical thinker. Whenever you see opportunities in our conversations, please push my critical thinking ability. Now, AI will do it.

## Vectors and Text Embeddings

To finish off, I'm going to leave you with a slightly more complex subject: text embeddings and vectors. In computer science, particularly in the realm of machine learning and natural language processing (NLP), text embedding is a popular technique to represent textual information in a format that can be easily processed by algorithms.

In the context of prompt engineering, LLM embedding refers to representing prompts in a form that the model can understand and process. This involves converting the text prompt into a high-dimensional vector that captures its semantic information.

Why do this? Think about it this way: if you ask a computer to come back with a similar word to "food," you wouldn't really expect it to come back with "burger" or "pizza," right? That's what a human might do when thinking of similar words to food. A computer would more likely look at the word lexicographically (kind of like when you scroll through a dictionary) and come back with "foot," for example. This is kind of useless to us.

We want to capture a word's semantic meaning - the meaning behind the word. Text embeddings do essentially that. Thanks to the data captured in a super long array of numbers, I can find words that are similar to "food" in a large corpus of text by comparing text embedding to text embedding and returning the most similar ones. So words such as "burger" instead of "foot" will be more similar.

## The Future of Prompt Engineering: Model Context Protocol and Agentic AI

Model Context Protocol (MCP) really is a big deal, but most people are missing the point here. Everybody's talking about enhancing desktop applications with agentic functionality. But if you want to write agentic AI applications at work like a professional, you're going to need a broader vision.

### Understanding Agentic AI Architecture

What we're doing is building an agent - you could think of it as a microservice. There's nothing particularly exotic about this. But in MCP terms, this is called the host application. And the host application uses the MCP client library to create an instance of a client in there.

Out here, we're going to create an MCP server. This may be a server that already exists that somebody else has built that we want to take advantage of to bring agentic functionality into our service, or this could be a server that we ourselves are creating. Inside the server, what do we have? Well, we've got access to tools, resources, prompts, capabilities that the server makes available and even describes to the outside world.

So this is a server process. There's a URL, port, etc, and a variety of well-known RESTful endpoints described by the MCP specification that are implemented by this server, including this capabilities list that tells the world, tells the host application, tells the client, whether there are tools present, what sort of resources might be available, what prompts it has, etc.

### How MCP Works in Practice

Let's walk through an example. Let's say we're building a service for making appointments. Sort of generalized meet with somebody, some group of people at some place, and not necessarily a conference room in the office, but maybe we're getting coffee. Maybe we're getting breakfast. Maybe it's a romantic dinner with your spouse.

I've just described a number of tools and resources that are necessary to make that happen. I need to create a calendar invite. I need some kind of calendar API integration. I need to see at least when my calendar is free, I might need to make assumptions about the counterparty. Maybe I can get access to their calendar as well, depending on what I've got permissions for. That would be kind of cool.

Places I might meet - I suppose knowing about the calendar might better fit under Resource. Tool would be making the appointment, maybe making a reservation at a restaurant, knowing what restaurants, what coffee shops, what breakfast joints are in the area. These are resources that I wanna make available to my agentic application.

Here's how the workflow might work:

1. **Initial Prompt**: A prompt comes in from the user: "I wanna have coffee with Peter next week."

2. **Capability Discovery**: The host application asks, "What capabilities do you have?" It knows the URL of this agent and can interrogate the capabilities to see what resources are available.

3. **Resource Assessment**: The client takes the prompt and asks the LLM: "You know what, on pass number one, I'll say, here is what my user

---

## Article Metadata

**Topic:** context engineering

**Generated:** 2025-10-23 09:22:15

**Sources:**

1. [Prompt Engineering Tutorial – Master ChatGPT and LLM Responses](https://www.youtube.com/watch?v=_ZvnD73m40o)
   - Channel: freeCodeCamp.org
   - Views: 2,408,496
   - Comments: 1,083

2. [Why MCP really is a big deal | Model Context Protocol with Tim Berglund](https://www.youtube.com/watch?v=FLpS7OfD5-s)
   - Channel: Confluent Developer
   - Views: 580,942
   - Comments: 743

3. [Stanford's Practical Guide to 10x Your AI Productivity | Jeremy Utley](https://www.youtube.com/watch?v=yMOmmnjy3sE)
   - Channel: EO
   - Views: 406,198
   - Comments: 432

**Cost Summary:**

- Total Input Tokens: 25,912
- Total Output Tokens: 16,641
- Total Tokens: 42,553
- **Total Cost: $0.3274**
- Model: Claude Sonnet 4

