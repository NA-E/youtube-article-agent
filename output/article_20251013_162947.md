# Master Prompt Engineering: Your Guide to Getting Perfect Responses from ChatGPT and LLMs

Hi everyone and welcome to this comprehensive guide on prompt engineering. My name is Ania Kubo and I'm a software developer as well as course creator here on freeCodeCamp. This is going to be a unique course for me as there is going to be a lot less coding going on but a lot more understanding about the topic of prompt engineering and why some companies are paying up to $335,000 a year according to Bloomberg for people in this profession. And no, no coding background is necessarily required. So what are we waiting for? Let's do it.

## What Exactly Is Prompt Engineering?

Prompt engineering in a nutshell is a career that came about of the back of the rise of artificial intelligence. It involves human writing, refining and optimizing prompts in a structured way. This is done with the intention of perfecting the interaction between humans and AI to the highest degree possible. 

Not only that, however, a prompt engineer is then also required to continuously monitor those prompts ensure their effectiveness with time as AI progresses. Maintaining an up-to-date prompt library will be a requirement that is placed onto the prompt engineer as well as reporting on findings and in general, being a thought leader in this space.

But why do we need it? And how did it come about from AI?

## Understanding AI: The Foundation

Before moving on, let's actually make sure we are on the same page about what exactly AI is. Artificial intelligence is the simulation of human intelligence processes by machines. I say simulation as artificial intelligence is not sentient, at least not yet anyways, meaning it cannot think for itself as much as it may seem it does.

Often, and this is certainly the case with tools such as ChatGPT, for example, when we say AI, we are simply referring to a term called machine learning. Machine learning works by using large amounts of training data that is then analyzed for correlations and patterns. These patterns are then used to predict outcomes based on the training data provided.

So for example, here we are feeding data saying that if a paragraph looks like this with this type of title, then it should be categorized as international finance. The second paragraph should be put in the category of earning reports and so on. With some code, we should be able to train our AI model to correctly guess what future paragraphs are about. And that's it.

## Why Prompt Engineering Is Essential

With the quick and exponential growing rise of AI, even the architects of it themselves struggle to control it and its outputs. This might be a bit hard to understand, but think of it this way. If you were to ask an AI chatbot what is four plus four, you would expect it to say eight, right? The result of eight is pretty indisputable.

However, imagine you are a young student trying to learn the English language. I'm going to show you just how different responses can be based on the prompts you feed and in turn your learning experience.

### The Basic Approach vs. The Engineered Approach

If you were to type, "correct my paragraph" and then paste it a badly written paragraph like "today was great in the world for me. I went to a Disneyland with my mom. It could have been better though if it wasn't raining." Great, the young English learner has a better sentence, but it kind of stops there and the learner is just left to their own devices. And honestly, the sentence really isn't that great anyway.

What if the learner could get the best sentences possible from a teacher who understands their interests to keep them engaged? With the correct prompts, we can actually create that with AI.

Here's the engineered prompt:

*"I want you to act as a spoken English teacher. I will speak to you in English and you'll reply to me in English to practice my spoken English. I want you to keep my reply neat, limiting the reply to 100 words. I also want you to strictly correct my grammar mistakes and typos. And I want you to ask me a question in your reply. Now, let's start practicing. You could ask me a question first. Remember, I want you to strictly correct my grammar mistakes and typos."*

Now this is way more interactive. As you will see, it's asking you a question and telling you what to do and will provide you with corrections if needed. So in a way, you're communicating with the AI. It's giving you suggestions and you're learning along the way. It's a completely different experience thanks to the prompt that we wrote. Pretty cool, right?

## The Role of Linguistics in Prompt Engineering

Linguistics is the study of language. It focuses on everything from phonetics, so the study of how speech sounds are produced and perceived. Phonology, so the study of sound patterns and changes. Morphology, the study of word structure. Syntax, so the study of sentence structure. Semantics, so the study of linguistic meaning. Pragmatics, so in other words, the study of how language is used in context. Historical linguistics, or the study of language change. Sociolinguistics, or in other words, the study of the relation between language and society. Computational linguistics, so the study of how computers can process human language. And physiolinguistics, or the study of how humans acquire and use language.

Linguistics are the key to prompt engineering. Why? Understanding the nuances of language and how it is used in different contexts is crucial for crafting effective prompts. Not only that, but knowing how to use a grammar or language structure that is universally used will result in the AI system returning back the most accurate results.

## Understanding Language Models

Imagine a world where computers possess the power to understand and generate human language. A world where machines can chat, write stories, and even compose poetry. In this magical realm, language models can come into play. They are like the wizards of the digital realm capable of understanding and creating human-like text.

A language model is a clever computer program that learns from a vast collection of written text. It takes in books, articles, websites, and all sorts of written resources, allowing it to gather knowledge about how humans use language. Just like a master linguist, it becomes an expert in the art of conversation, grammar, and style.

### How Language Models Work

When you feed a sentence to a language model, it will analyze the sentence, examine the order of words, their meanings, and the way they fit together. Then the language model would generate a prediction or a continuation of the sentence that makes sense based on its understanding of the language. It will weave words together one by one, creating a response that seems like it was crafted by a human being.

## A Brief History: From ELIZA to GPT-4

Let's start off by looking at the history of language models, starting with the first AI, ELIZA, back in the 60s. ELIZA is an early natural language processing computer program created from 1964 to 1966 at MIT by Joseph Weizenbaum. ELIZA was designed to simulate a conversation with a human being.

ELIZA had a special knack for mimicking a Rogerian psychotherapist, someone who essentially listens attentively and asks probing questions to help people explore their thoughts and feelings. ELIZA's secret weapon was its mastery of pattern matching. It had a treasure trove of predefined patterns, each associated with specific responses.

But here's the delightful twist. ELIZA didn't truly understand what you were saying. It was just a clever illusion. It used pattern matching and some creative programming tricks to create the illusion of understanding while in reality, it was just following a set of predefined rules.

Fast forward to the 1970s, when a program named SHRDLU appeared. It could understand simple commands and interact with a virtual world of blocks. Although SHRDLU wasn't a language model per se, it laid the foundation for the idea of machines comprehending human language.

But the true language models began around 2010, when the power of deep learning and neural networks came into play. Enter the mighty GPT, short for generative pre-trained transformer, ready to conquer the world of language.

In the year 2018, the first iteration of GPT emerged, created by the company OpenAI. GPT-1 was a taste of things to come, an impressive language model, but small compared to its descendants that we use today.

As time went on, the saga continued with the arrival of GPT-2 in 2019, followed by GPT-3 in 2020. This was a titan among language models equipped with a large number of parameters, over 175 billion to be precise. GPT-3 dazzled the world with its unparalleled ability to understand, respond, and even generate creative pieces of writing.

At the time of writing, we now also have GPT-4, trained on pretty much the whole internet, rather than outdated large data sets, as well as BERT from Google and so much more. It would seem we are only just at the start when it comes to language models and AI. So learning how to harness this data with prompt engineering is a smart move for anyone today.

## The Prompt Engineering Mindset

When thinking of good prompts, it is always best to get in the correct mindset. Essentially, you want to just write one prompt, right? And not have to waste time and tokens, writing lots of different prompts until you get the result you desire.

So essentially, kind of the same as when you Google stuff, right? How good are your Googling skills now, as opposed to five years ago? I'm assuming a lot better. We have grown to intuitively know what to type into Google the first time round as to not waste time. Having the same mindset for prompt engineering can also be applied.

Mihail Eric of the Infinite Machine Learning Podcast says it well when he says, "I personally like the analogy of prompting to designing effective Google searches. There are clearly better and worse ways to write queries against the Google search engine that solve your task. This variance exists because of the opaqueness of what Google is doing under the hood."

## Understanding Tokens and Context Windows

GPT-4 essentially processes all texts in chunks called tokens. And this token is approximately four characters or 0.75 words for English text. And we essentially get charged by token. If you want to know exactly how many tokens you are using, you can check it out with the tokenizer tool, and it will give you a rough example.

So for example, if you say "what is four plus four," with that piece of text, the total count of tokens is going to be six. That is exactly how many tokens will be used in order to produce an answer for this request.

### The Critical Role of Context Windows

Here's where it gets really interesting - and why sometimes LLMs like ChatGPT get kind of dumb. You'll be deep into a conversation that you can't even scroll to the top of because it's so long and it starts to say weird things and hallucinate. It forgets what you're talking about. It makes stuff up and it's stinking slow. Why is this happening? Context windows.

LLMs like ChatGPT, Gemini, Claude, even local models like Llama or DeepSeek, they're kind of like us in that they have memories. Short-term memory, which means they can remember things, but also sometimes they can forget stuff. Let's say you and me we're having some coffee and we're talking for about 15 minutes and that short amount of time, we remember pretty much everything. But you're pretty fun to talk to. So we end up talking for an hour, two hours, three hours, and at that point it's kind of hard to keep track of stuff. I forget the amazing point you made. And sometimes we forget the entire point of the conversation. We've talked so long.

ChatGPT does the same thing. The longer that conversation goes on, the more things you say, the more things it says back to you. It has to store all of that in its short-term memory. And that short-term memory has a limit. That limit is its context window.

### Context Window Sizes Across Models

Different models have different context window capabilities:

- GPT-4 is rocking 128,000 tokens
- Claude 3.5: 200,000 tokens  
- Gemini 2.5 from Google: 1 million tokens
- Meta's Llama 4 Scout: 10 million tokens

But here's the catch. Even if you have a super large context window, it doesn't mean the LLM won't kind of freak out and forget stuff, become less accurate or start to go extremely slow. You'll notice on those larger conversations, it'll have some trouble paying attention.

### The "Lost in the Middle" Problem

There was a paper released called "Lost in the Middle," and it showed us how LLMs are kind of like us with paying attention. Just like watching a long movie where you watch the first part, then fall asleep and then wake up at the end - conversations with LLMs with large context show the same pattern. The models were more accurate with info at the beginning and even with info at the end. But in the middle, huge drop off. And across the board we saw this U shape. So LLMs are falling asleep during our conversation.

### How LLMs Pay Attention

When you say something to an LLM like, "Hey, I want coffee, but caffeine makes me jittery. What should I get?" it will do something really similar to how we process and think. It'll use some fancy semantic math to decide which of these words is important, which is relevant both to the context of your entire conversation and to how the words relate to each other. It might assign attention scores saying, "Hey, in this conversation, coffee is high, caffeine is high, jittery," but words like "I" or "me," kind of low relevance to the context.

This process is pretty complex. All this math to assign attention scores, and it does this every time you send something to the LLM, every time you add to the conversation. Those larger contexts not only have insane memory requirements, they also have some pretty healthy computational requirements. Every time you add to that conversation, that math problem that has to run to figure out what's important gets bigger and it requires more power. And that is why in those larger conversations, the LLM starts to hallucinate and it seems just a bit more slow.

### Practical Context Window Management

Here's a rule to go by when talking with LLMs: When you change an idea, when it's a significant shift from what you're currently talking about, start a new chat. The performance will be so much better. In fact, sometimes when you're talking with other LLMs like Claude, it'll even tell you at the bottom, "Hey, you've been talking for a minute. Things are going to slow down. Why don't you go ahead and start a new chat so things can be better."

## The Model Context Protocol (MCP): Revolutionizing AI Integrations

One of the most exciting developments in the AI integration space is the Model Context Protocol, or MCP. As Alex from Anthropic explains, "MCP is just a way for putting my workflow into like an AI applications in a very simple way." It's a standardized protocol that allows AI applications to connect with external data sources, tools, and services seamlessly.

### Understanding MCP's Core Components

MCP standardizes how you take data from APIs or internal data sources and actually give it to the model. The protocol exposes three main things:

1. **Tools** - Actions that the model can take out in the world
2. **Resources** - Raw data like files, texts, or any context you want to give the model
3. **Prompts** - Prompt templates that can be triggered by users, often implemented as slash commands

### The Origin Story: From Copy-Paste Frustration to Industry Standard

The origin of MCP came from a very relatable problem. As David, one of the co-creators, explains: "I worked on like internal developer stuff, and I got very quickly frustrated about like having to copy things in and out of Claude desktop and then copying things back and forth between my IDE, and that's just really what I would thinking about, like how can I solve copy and pasting the things I care about the most between these two applications."

What started as a solution to copy-paste frustration became something much bigger. During an internal hackathon at Anthropic in September, something remarkable happened. As Theo recalls: "Everyone was free to build basically whatever they wanted to build. But it turns out everyone just built an MCP... Everyone's ideas were, 'Oh, but what if we made this an MCP server?'"

The organic adoption was incredible - from standard integrations like Slack to creative projects where people controlled 3D printers with MCP. This wasn't mandated; it just naturally happened because MCP made it so much easier to add context to AI applications.

### Why MCP Became an Open Standard

The decision to make MCP open source was strategic. As explained by the team: "If you have a closed ecosystem for integrations and for a context to be provided to AI applications, then it isn't clear to the server builders or the integration builders - is that AI application gonna be around forever? Should they invest in that?"

By making it an open standard, they decreased the friction to building integrations and allowed the industry to focus on what really matters: the model's intelligence and the workflow you build on top of it, rather than just which integrations you have access to.

### The Magic Moment of MCP

There's something special that happens when you first use MCP. As David puts it: "I think there's a bit of a magic moment when you teach Claude something new using an MCP server for the first time, and you see it takes action about something you care about." It's that moment when Claude goes from just outputting text to actually doing things - calling applications, fetching data, or even operating physical devices.

### Current State and Adoption

Since launching around Thanksgiving 2024, MCP has grown to include:

- Major players adopting it across their products
- An ecosystem of 10,000+ MCP server builders
- Evolution from primarily local experiences to cloud-hosted servers through "remote MCP"
- Claude AI integrations that allow connecting websites offering MCP servers directly into workflows

### Creative MCP Applications

The creativity in MCP implementations has been remarkable. Some favorites include:

- **Music and Hardware Control**: Servers that control synthesizers, allowing Claude to interact with physical devices that make music
- **Creative Software Integration**: Blender integration where Claude writes scripts and creates 3D scenes in real-time
- **Smart Home Integration**: Team members have Claude control doors and role-play as a doorman
- **Real-World Applications**: From issue trackers to 3D printers, the possibilities are endless

### Getting Started with MCP

For developers new to MCP, the recommendation is straightforward:

1. **Start by exploring**: Look at existing servers online and play with them in Claude AI or Claude desktop
2. **Begin simple**: Start with a basic "hello world" - just one tool that responds with "Hello world"
3. **Use Claude to help**: Paste the MCP documentation into Claude and ask it to make you a server
4. **Learn from examples**: Look at great existing servers and modify them for your needs

As the team notes: "With like 10 minutes you can have something" up and running.

### The Future of MCP

The roadmap for MCP includes several exciting developments:

1. **Registry API**: Allowing models to search for and bring in additional servers on demand, enabling more agentic behavior
2. **Long-running tasks**: Making it easier to handle extended operations
3. **Elicitation**: Enabling servers to ask users for more information when needed
4. **Enhanced security primitives**: Better identity and authorization for enterprise deployments

As AI models become more capable, like with the recent release of Claude 4, some MCP primitives that haven't seen much adoption yet will become more important. Features related to statefulness and sampling will become more valuable as models can handle longer-running, more complex tasks.

## Best Practices for Effective Prompting

The biggest misconception when it comes to prompt engineering is that it's an easy job with no science to it. I imagine a lot of people think it's just about constructing a one-off sentence such as "correct my paragraph" that we saw in the previous example.

When you start to look at it, creating effective prompts relies on a bunch of different factors. Here are some things to consider when writing a good prompt:

### 1. Write Clear Instructions with Details

Consider writing clear instructions with details in your query. To get the best results, don't assume the AI knows what you are talking about. Writing something like, "when is the election," implies that you are expecting the AI to know what election you are talking about and what country you mean. This may result in you asking a few follow-up questions to finally get the result you want, resulting in time loss and frankly, perhaps some frustration.

**Bad example:** "When is the election?"
**Good example:** "When is the next presidential election for Poland?"

Here's another comparison:

**Vague prompt:** "Write code to filter out the ages from data."
**Clear prompt:** "Write a JavaScript function that will take an array of objects and filter out the value of age property and put them in a new array. Please explain what each code snippet does."

In this example, I am not assuming the AI knows what computer language I like to use and I am being more specific about what my data actually looks like. Not only that, I'm also asking the AI to explain why it's doing each step so that I in turn can understand and not just copy paste the code without gaining any knowledge from it.

### 2. Adopt a Persona

When writing prompts, it is sometimes helpful to create a persona. This means you're asking the AI to respond to you and a certain character. Using a persona in prompt engineering can help ensure that the language model's output is relevant, useful and consistent with the needs and preferences of the target audience.

**Basic prompt:** "Write a poem for a sister's high school graduation that will be read out to a family and close friends."

**Persona-enhanced prompt:** "Write a poem as Helena. Helena is 25 years old and an amazing writer. Her writing style is similar to famous 21st century poet Rupi Kaur. Writing as Helena, write a poem for her 18 year old sister to celebrate her sister's high school graduation. This will be read out to friends and family at the gathering."

The second version produces a much more refined, personal poem that captures a specific voice and style.

### 3. Specify the Format

We can specify if something is a summary, a list or a detailed explanation. You can even create checklists. Just make sure to specify the type of format you want and it should be able to do it.

For example, instead of just asking "tell me what this essay is about," you could specify: "Use bullet points to explain what this essay is about, making sure each point is no longer than 10 words long."

### 4. Use Iterative Prompting

If you have a multi-part question or if the first response wasn't sufficient, you can continue by asking follow-up questions or asking the model to elaborate.

### 5. Avoid Leading the Answer

Try not to make your prompts so leading that it inadvertently tells the model what answer you're expecting. This might bias the response unduly.

### 6. Limit the Scope for Long Topics

If you're asking about a broad topic, it's helpful to break it down or limit the scope to get a more focused answer.

### 7. Mind Your Context Window Usage

A lot of things can fill up the context window. The obvious things are things we say and the things it says back, but there also might be system prompts which are instructions for our LLMs. You also might give it documents. You might paste a PDF or an Excel spreadsheet that'll take up some more tokens. And when you're doing some live coding, the code is taking up tokens filling up that context window.

## Zero-Shot vs. Few-Shot Prompting

### Zero-Shot Prompting

Zero-shot prompting leverages a pre-trained model's understanding of words and concept relationships without further training. So essentially in the context of the GPT-4 model, we don't really need to do much. We are already using all of the data that it has.

For example, asking "When is Christmas in America?" is zero-shot prompting because the model already has this information from its training data.

### Few-Shot Prompting

Few-shot prompting enhances the model with training examples via the prompt avoiding retraining. Instead of zero examples, we give it a tiny bit of data.

For example, if I want ChatGPT to know my food preferences:

First, I provide examples: "Ania's favorite type of food includes burgers, fries, and pizza."

Then I can ask: "What restaurant should I take Ania to in Dubai this weekend?"

The model will now use the context I provided about my food preferences to make better recommendations. This is a great example of few-shot prompting in which it wouldn't have been able to answer this question if I hadn't given it some example data or just trained the model a little bit more in order to get the response that I want.

## Understanding AI Hallucinations

Now we're delving into something you probably never thought you'd hear in an AI context and that's hallucinations. So what exactly are AI hallucinations? And no, they're not when your AI assistants start seeing unicorns and rainbows. It's actually a term that refers to the unusual outputs that AI models can produce when they misinterpret data.

AI hallucinations can happen with text models. An example of this is us asking a text model about a historical figure and the text model not having an answer and hallucinating one instead, resulting in an inaccurate response.

Why do these hallucinations happen? AI models are trained on a huge amount of data and they make sense of new data based on what they've seen before. Sometimes, however, they make connections that are, let's call it creative. And voila, an AI hallucination occurs.

Despite their funny results, AI hallucinations aren't just entertaining. They're also quite enlightening. They show us how our AI models interpret and understand data. It's like a sneak peek into their thought processes.

Hallucinations become more common in longer conversations where the model struggles to maintain attention across the entire context window, especially when information gets "lost in the middle" of very long conversations.

## Vectors and Text Embeddings

To finish off, I'm going to leave you with a slightly more complex subject. We're going to take a quick look at the topic of text embedding and vectors.

In computer science, particularly in the realm of machine learning and natural language processing or NLP, text embedding is a popular technique to represent textual information in a format that can be easily processed by algorithms, especially deep learning models.

In the context of prompt engineering, LLM embedding refers to representing prompts in a form that the model can understand and process. This involves converting the text prompt into a high dimensional vector that captures its semantic information.

### Why Use Text Embeddings?

Think about it this way. If you ask a computer to come back with a similar word to "food," you wouldn't really expect it to come back with "burger" or "pizza," right? That's what a human might do when thinking of similar words to food. A computer would more likely look at the word lexicographically, kind of like when you scroll through a dictionary and come back with "foot," for example. This is kind of useless to us.

We want to capture a word's semantic meaning. So the meaning behind the word. Text embeddings do essentially that, thanks to the data captured in a super long array of numbers. Now, I can find words that are similar to "food" in a large corpus of text by comparing text embedding to text embedding and returning the most similar ones. So words such as "burger" instead of "foot" will be more similar.

## Advanced Context Window Optimizations

For those working with local models or wanting to understand the technical limitations better, there are several optimizations available to maximize context window usage:

### Flash Attention

Flash attention is an optimization that changes how the model computes its attention and assigns those weights. It's doing the same kind of semantic math that regular self attention mechanisms are doing, but it's like that lazy kid in class who always got straight A's. It'll actually skip building the full table of token comparisons by processing tokens in chunks with optimized GPU routines. This leads to significant improvements in not only memory but speed.

### Cache Optimizations

KC and V cache optimizations compress data so it takes up less room in video memory. These optimizations can allow you to use much larger context windows without maxing out your GPU memory.

### The Memory Trade-offs

Setting high values for context length can significantly impact memory usage. While models might officially support massive context windows (like 128K tokens), your hardware might not be able to handle the full context, specifically when it comes to VRAM or video RAM. Bigger context windows require more compute power and GPU resources in more ways than one.

## Security Considerations with Large Context Windows

There's a scarier downside to larger context windows that's worth mentioning: the larger attack surface. LLMs can be hacked and they're vulnerable to some creative prompting that can jailbreak out of their protection systems. The longer a conversation is, the more it can kind of forget what's in the middle, and the easier it will be for an attacker to hide some malicious stuff in there that could bypass its safety precautions.

## Conclusion

Thank you so much for reading this comprehensive guide on prompt engineering. We've covered what it is, an introduction to AI as well as looked at linguistics, language models, prompt engineering mindset, using GPT-4, how to look at best practices as well as zero-shot and few-shot prompting, AI hallucinations, context windows and their limitations, vectors and text embeddings, and the revolutionary Model Context Protocol that's changing how we integrate AI with our workflows.

The key takeaway is that prompt engineering is both an art and a science. It requires understanding how language models work, being specific and clear in your instructions, managing context windows effectively, and continuously refining your approach. As AI continues to evolve, mastering these skills will become increasingly valuable.

With tools like MCP making it easier than ever to connect AI to real-world applications and data sources, we're entering an era where the possibilities are truly endless. From controlling synthesizers to managing 3D printers, from automating workflows to creating interactive learning experiences, the future of human-AI interaction is being written right now.

Remember, just like developing good Google search skills took time and practice, becoming proficient at prompt engineering requires patience and experimentation. Start with the best practices we've covered, be mindful of context window limitations, and don't be afraid to iterate and improve your prompts based on the results you get.

Understanding the technical limitations like context windows and attention mechanisms will make you a more effective prompt engineer. When you notice a conversation getting long and the AI starting to lose focus, don't hesitate to start a new chat. When you're switching topics significantly, start fresh. These simple practices can dramatically improve your results.

The future of human-AI interaction lies in our ability to communicate effectively with these powerful tools, and prompt engineering - combined with an understanding of how these systems actually work and how to extend them through protocols like MCP - is your gateway to unlocking their full potential.

---

## Article Metadata

**Topic:** context engineering

**Generated:** 2025-10-13 16:29:47

**Sources:**

1. [Prompt Engineering Tutorial – Master ChatGPT and LLM Responses](https://www.youtube.com/watch?v=_ZvnD73m40o)
   - Channel: freeCodeCamp.org
   - Views: 2,396,159
   - Comments: 1,083

2. [Why LLMs get dumb (Context Windows Explained)](https://www.youtube.com/watch?v=TeQDr4DkLYo)
   - Channel: NetworkChuck
   - Views: 150,630
   - Comments: 319

3. [The Model Context Protocol (MCP)](https://www.youtube.com/watch?v=CQywdSdi5iA)
   - Channel: Anthropic
   - Views: 207,167
   - Comments: 241

**Cost Summary:**

- Total Input Tokens: 28,559
- Total Output Tokens: 17,148
- Total Tokens: 45,707
- **Total Cost: $0.3429**
- Model: Claude Sonnet 4

